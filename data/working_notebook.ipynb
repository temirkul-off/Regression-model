{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main_df = pd.read_csv('train_main_data.csv')\n",
    "train_additional_df = pd.read_csv('train_additional_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_main_df = pd.read_csv('test_main_data.csv')\n",
    "test_additional_df = pd.read_csv('test_additional_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>max_floor</th>\n",
       "      <th>material</th>\n",
       "      <th>build_year</th>\n",
       "      <th>num_room</th>\n",
       "      <th>kitch_sq</th>\n",
       "      <th>apartment condition</th>\n",
       "      <th>sub_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74544</td>\n",
       "      <td>2014-03-11</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78384</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30355</td>\n",
       "      <td>2012-08-16</td>\n",
       "      <td>29</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16306</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>55</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48126</td>\n",
       "      <td>2013-03-07</td>\n",
       "      <td>32</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   timestamp  full_sq  life_sq  floor  max_floor  material  \\\n",
       "0  74544  2014-03-11       39      NaN    6.0        1.0       1.0   \n",
       "1  78384  2014-12-31       34      NaN    2.0       17.0       1.0   \n",
       "2  30355  2012-08-16       29     18.0    2.0        NaN       NaN   \n",
       "3  16306  2013-05-02       55     37.0    3.0        NaN       NaN   \n",
       "4  48126  2013-03-07       32     16.0    9.0        NaN       NaN   \n",
       "\n",
       "   build_year  num_room  kitch_sq  apartment condition  sub_area  \n",
       "0         NaN       1.0       1.0                  1.0        72  \n",
       "1         NaN       1.0       0.0                  NaN        86  \n",
       "2         NaN       NaN       NaN                  NaN        12  \n",
       "3         NaN       NaN       NaN                  NaN       118  \n",
       "4         NaN       NaN       NaN                  NaN        28  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main_df = train_main_df.sort_values(by='timestamp')\n",
    "train_ids = train_main_df.id.drop_duplicates()\n",
    "train_df = train_main_df[:round(len(train_ids) * 0.7)]\n",
    "valid_df = train_main_df[round(len(train_ids) * 0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20300, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8700, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендуемые этапы анализа данных\n",
    "- Preprocessing(missing values,data type ...)\n",
    "- EDA(univariate and muiltuvariate analysis)\n",
    "- Feature engineering\n",
    "\n",
    "- Hypertuning(protuning of all available params)\n",
    "- Feature selection\n",
    "- Post analysis on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccessing(df,add_df,model_for_na=None):\n",
    "    \n",
    "    df = df[df['full_sq'] > 0]\n",
    "    df = df.merge(add_df, how= 'left', on='id')\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['year'] = df['timestamp'].dt.year.astype(int)\n",
    "    df['month'] = df['timestamp'].dt.month.astype(int)\n",
    "    if model_for_na:\n",
    "        no_life_sq = model_for_na.predict(df[df.life_sq.isnull()].full_sq.values.reshape(-1,1))\n",
    "        df['life_sq_predicted'] = df.life_sq\n",
    "        df.loc[df.life_sq.isnull(),'life_sq_predicted'] = list(no_life_sq.reshape(1,-1)[0])\n",
    "    else:\n",
    "        df['life_sq'].fillna(0,inplace =True)\n",
    "    \n",
    "    df = df.fillna(-20) # since all values are positive, negative values is set for NAs so model could distinguish them\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = preproccessing(train_df, train_additional_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = preproccessing(valid_df,train_additional_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_gen(df):\n",
    "    # get week of the year\n",
    "    df['week_of_year'] = df['timestamp'].dt.isocalendar().week.astype(int)\n",
    "    \n",
    "    # get day of week\n",
    "    df['day_of_week'] = df['timestamp'].dt.weekday\n",
    "    \n",
    "    #df['timestamp_int'] = df['timestamp'].astype(int)\n",
    "    \n",
    "    # get ratio of squares\n",
    "    df[\"ratio_life_dash_full_sq\"] = df[\"life_sq\"] / df[\"full_sq\"]\n",
    "    df[\"ration_kitchen_dash_full_sq\"] = df[\"kitch_sq\"] / df[\"full_sq\"]\n",
    "    \n",
    "    \n",
    "    # age of building\n",
    "    df['age'] = df[\"build_year\"] - df['year']\n",
    "    \n",
    "    # difference between full area and living area\n",
    "    df['some_extra_sqr_1'] = df[\"full_sq\"] - df[\"life_sq\"]\n",
    "    if \"life_sq_predicted\" in df.columns:\n",
    "        df['some_extra_sqr_2'] = df[\"full_sq\"] - df['life_sq_predicted']\n",
    "    df.drop(columns=['timestamp'],inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_set_1  = feature_gen(training_set)\n",
    "valid_set_1  = feature_gen(valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lin_model:\n",
    "    def __init__(self, degree=1, regularization = None, lambda_=0):\n",
    "        if regularization:\n",
    "            # self.linear_model = Lasso(alpha=lambda_)\n",
    "            self.linear_model = Ridge(alpha=lambda_)\n",
    "        else:\n",
    "            self.linear_model = LinearRegression()\n",
    "        \n",
    "        # self.encoder = OneHotEncoder()\n",
    "        self.poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X_train,y_train,X_valid,y_valid,score=False):\n",
    "        ''' just fits the data. mapping and scaling are not repeated '''\n",
    "        # X_train_encoded = self.encoder.fit_transform(X_train)\n",
    "        X_train_mapped = self.poly.fit_transform(X_train)\n",
    "        X_train_mapped_scaled = self.scaler.fit_transform(X_train_mapped)\n",
    "        \n",
    "        self.linear_model.fit(X_train_mapped_scaled, y_train)\n",
    "        yhat_valid = self.predict(X_valid,preprocessed=False) \n",
    "        yhat_train = self.predict(X_train_mapped_scaled) \n",
    "        if score:\n",
    "            valid_score = self.scores(y_valid,yhat_valid,name='_valid')\n",
    "            train_score = self.scores(y_train,yhat_train,name='_train')\n",
    "            print(pd.concat([train_score,valid_score],axis=1))\n",
    "            # return pd.concat([train_score,valid_score],axis=1)\n",
    "            \n",
    "    def predict(self, X,preprocessed=True):\n",
    "        # X_encoded = self.encoder.transform(X)\n",
    "        if not preprocessed:\n",
    "            X_mapped = self.poly.transform(X)\n",
    "            X = self.scaler.transform(X_mapped)\n",
    "             \n",
    "        yhat = self.linear_model.predict(X)\n",
    "        return(yhat)\n",
    "    \n",
    "    def scores(self, y, yhat,name=''):\n",
    "        mse = mean_squared_error(y,yhat)/2   #sklean doesn't have div by 2\n",
    "        rms = mean_squared_error(y, yhat, squared=False)/2\n",
    "        # print()\n",
    "        return pd.DataFrame({f'RMSE{name}':[rms],f'MSE{name}':[mse]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RMSE_train     MSE_train    RMSE_valid     MSE_valid\n",
      "0  1.597508e+06  5.104065e+12  2.092741e+06  8.759129e+12\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lin_model.fit(training_set_1.drop(columns='price'),\n",
    "              training_set_1['price'],\n",
    "              valid_set_1.drop(columns='price'),\n",
    "              valid_set_1['price'],\n",
    "              score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taske regarding above model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Deal with overfitting (variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Network for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,linear\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "class NN_model:\n",
    "    def __init__(self, degree=3, regularization = None, lambda_=0):\n",
    "        if regularization:\n",
    "            pass\n",
    "            # self.linear_model = Lasso(alpha=lambda_)\n",
    "            # self.linear_model = Ridge(alpha=lambda_)\n",
    "        else:\n",
    "            RegularizedDense = partial(Dense,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=tf.keras.regularizers.L2(l2=0.01))\n",
    "            self.model = Sequential([\n",
    "                                    RegularizedDense(25),\n",
    "                                    RegularizedDense(15),\n",
    "                                    RegularizedDense(1, activation=\"linear\")],\\\n",
    "                                    # kernel_initializer=\"glorot_uniform\"\n",
    "                                                     \n",
    "                                    name='NN_model')\n",
    "            self.model.compile(\n",
    "                                    loss='mean_squared_error',\n",
    "                                    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                                )\n",
    "             \n",
    "\n",
    "\n",
    "        \n",
    "        # self.encoder = OneHotEncoder()\n",
    "        self.poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, X_train,y_train,X_valid,y_valid,score=False):\n",
    "        ''' just fits the data. mapping and scaling are not repeated '''\n",
    "        # X_train_encoded = self.encoder.fit_transform(X_train)\n",
    "        X_train_mapped = self.poly.fit_transform(X_train)\n",
    "        X_train_mapped_scaled = self.scaler.fit_transform(X_train_mapped)\n",
    "           \n",
    "        self.model.fit(X_train_mapped_scaled, y_train,epochs=40)\n",
    "        yhat_valid = self.predict(X_valid,preprocessed=False) \n",
    "        yhat_train = self.predict(X_train_mapped_scaled) \n",
    "        if score:\n",
    "            valid_score = self.scores(y_valid,yhat_valid,name='_valid')\n",
    "            train_score = self.scores(y_train,yhat_train,name='_train')\n",
    "            print(pd.concat([train_score,valid_score],axis=1))\n",
    "            # return pd.concat([train_score,valid_score],axis=1)\n",
    "            \n",
    "    def predict(self, X,preprocessed=True):\n",
    "        # X_encoded = self.encoder.transform(X)\n",
    "        if not preprocessed:\n",
    "            X_mapped = self.poly.transform(X)\n",
    "            X = self.scaler.transform(X_mapped)\n",
    "             \n",
    "        yhat = self.model.predict(X)\n",
    "        return(yhat)\n",
    "    \n",
    "    def scores(self, y, yhat,name=''):\n",
    "        mse = mean_squared_error(y,yhat)/2   #sklean doesn't have div by 2\n",
    "        rms = mean_squared_error(y, yhat, squared=False)/2\n",
    "        # print()\n",
    "        return pd.DataFrame({f'RMSE{name}':[rms],f'MSE{name}':[mse]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "635/635 [==============================] - 7s 9ms/step - loss: 43428331126784.0000\n",
      "Epoch 2/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 34579364708352.0000\n",
      "Epoch 3/40\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 20634899841024.0000\n",
      "Epoch 4/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 14248932540416.0000\n",
      "Epoch 5/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 12313778192384.0000\n",
      "Epoch 6/40\n",
      "635/635 [==============================] - 7s 12ms/step - loss: 11502202388480.0000\n",
      "Epoch 7/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10996462649344.0000\n",
      "Epoch 8/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 10598998867968.0000\n",
      "Epoch 9/40\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 10266322403328.0000\n",
      "Epoch 10/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9970572591104.0000\n",
      "Epoch 11/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9712047226880.0000\n",
      "Epoch 12/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9482894573568.0000\n",
      "Epoch 13/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9325581959168.0000\n",
      "Epoch 14/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 9133299335168.0000\n",
      "Epoch 15/40\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 8962395078656.0000\n",
      "Epoch 16/40\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 8798253088768.0000\n",
      "Epoch 17/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 8650821206016.0000\n",
      "Epoch 18/40\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 8509643030528.0000\n",
      "Epoch 19/40\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 8385341161472.0000\n",
      "Epoch 20/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 8261582454784.0000\n",
      "Epoch 21/40\n",
      "635/635 [==============================] - 7s 10ms/step - loss: 8154216660992.0000\n",
      "Epoch 22/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 8039583186944.0000\n",
      "Epoch 23/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7946680926208.0000\n",
      "Epoch 24/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7845179293696.0000\n",
      "Epoch 25/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7752915615744.0000\n",
      "Epoch 26/40\n",
      "635/635 [==============================] - 5s 9ms/step - loss: 7666587926528.0000\n",
      "Epoch 27/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7580065726464.0000\n",
      "Epoch 28/40\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 7496706031616.0000\n",
      "Epoch 29/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7423418957824.0000\n",
      "Epoch 30/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7345287462912.0000\n",
      "Epoch 31/40\n",
      "635/635 [==============================] - 6s 10ms/step - loss: 7277530054656.0000\n",
      "Epoch 32/40\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 7202633940992.0000\n",
      "Epoch 33/40\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 7138539208704.0000\n",
      "Epoch 34/40\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 7073867759616.0000\n",
      "Epoch 35/40\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 7011704504320.0000\n",
      "Epoch 36/40\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 6946976432128.0000\n",
      "Epoch 37/40\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 6892312592384.0000\n",
      "Epoch 38/40\n",
      "635/635 [==============================] - 5s 8ms/step - loss: 6834872123392.0000\n",
      "Epoch 39/40\n",
      "635/635 [==============================] - 6s 9ms/step - loss: 6783552716800.0000\n",
      "Epoch 40/40\n",
      "635/635 [==============================] - 7s 11ms/step - loss: 6732790628352.0000\n",
      "272/272 [==============================] - 1s 4ms/step\n",
      "635/635 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[0;32mIn[32], line 42\u001b[0m, in \u001b[0;36mNN_model.fit\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, score)\u001b[0m\n\u001b[1;32m     40\u001b[0m yhat_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X_train_mapped_scaled) \n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score:\n\u001b[0;32m---> 42\u001b[0m     valid_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43myhat_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_valid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     train_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores(y_train,yhat_train,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mconcat([train_score,valid_score],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[32], line 57\u001b[0m, in \u001b[0;36mNN_model.scores\u001b[0;34m(self, y, yhat, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscores\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, yhat,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43myhat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m   \u001b[38;5;66;03m#sklean doesn't have div by 2\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     rms \u001b[38;5;241m=\u001b[39m mean_squared_error(y, yhat, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# print()\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/AI/tutoring/Intro_to_Data_science/Supervised/coursera_ml_sp/env/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/AI/tutoring/Intro_to_Data_science/Supervised/coursera_ml_sp/env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/Documents/AI/tutoring/Intro_to_Data_science/Supervised/coursera_ml_sp/env/lib/python3.9/site-packages/sklearn/metrics/_regression.py:110\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred have different number of output (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    112\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    113\u001b[0m         )\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    117\u001b[0m allowed_multioutput_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=10)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NN_model = NN_model()\n",
    "NN_model.fit(training_set_1.drop(columns='price'),training_set_1['price'],\\\n",
    "              valid_set_1.drop(columns='price'),valid_set_1['price'],\\\n",
    "              score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model.predict(image_of_two.reshape(1,400))  # prediction\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
